{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikravan1/3ML/blob/main/3ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOT8ICPPtOrb"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF python-docx python-pptx tiktoken\n",
        "!pip install gradio spaces\n",
        "!pip install bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/accelerate.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nikravan1/3ML.git"
      ],
      "metadata": {
        "id": "Z_KStEyEz7jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 3ML"
      ],
      "metadata": {
        "id": "OWAjlzzszvhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N_nW303FtFTY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import spaces\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
        "import os\n",
        "from threading import Thread\n",
        "\n",
        "import pymupdf\n",
        "import docx\n",
        "from pptx import Presentation\n",
        "\n",
        "\n",
        "MODEL_LIST = [\"nikravan/glm_4vq\"]\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
        "MODEL_ID = MODEL_LIST[0]\n",
        "MODEL_NAME = \"GLM-4vq\"\n",
        "\n",
        "TITLE = \"<h1>3ML-bot</h1>\"\n",
        "\n",
        "DESCRIPTION = f\"\"\"\n",
        "<center>\n",
        "<p>üòä A MultiModal MultiLingual(MMML) Chat.\n",
        "<br>\n",
        "üöÄ MODEL NOW: <a href=\"https://hf.co/{MODEL_ID}\">{MODEL_NAME}</a>\n",
        "</center>\"\"\"\n",
        "\n",
        "CSS = \"\"\"\n",
        "h1 {\n",
        "    text-align: center;\n",
        "    display: block;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def extract_text(path):\n",
        "    return open(path, 'r').read()\n",
        "\n",
        "\n",
        "def extract_pdf(path):\n",
        "    doc = pymupdf.open(path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_docx(path):\n",
        "    doc = docx.Document(path)\n",
        "    data = []\n",
        "    for paragraph in doc.paragraphs:\n",
        "        data.append(paragraph.text)\n",
        "    content = '\\n\\n'.join(data)\n",
        "    return content\n",
        "\n",
        "\n",
        "def extract_pptx(path):\n",
        "    prs = Presentation(path)\n",
        "    text = \"\"\n",
        "    for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                text += shape.text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def mode_load(path):\n",
        "    choice = \"\"\n",
        "    file_type = path.split(\".\")[-1]\n",
        "    print(file_type)\n",
        "    if file_type in [\"pdf\", \"txt\", \"py\", \"docx\", \"pptx\", \"json\", \"cpp\", \"md\"]:\n",
        "        if file_type.endswith(\"pdf\"):\n",
        "            content = extract_pdf(path)\n",
        "        elif file_type.endswith(\"docx\"):\n",
        "            content = extract_docx(path)\n",
        "        elif file_type.endswith(\"pptx\"):\n",
        "            content = extract_pptx(path)\n",
        "        else:\n",
        "            content = extract_text(path)\n",
        "        choice = \"doc\"\n",
        "        print(content[:100])\n",
        "        return choice, content[:5000]\n",
        "\n",
        "\n",
        "    elif file_type in [\"png\", \"jpg\", \"jpeg\", \"bmp\", \"tiff\", \"webp\"]:\n",
        "        content = Image.open(path).convert('RGB')\n",
        "        choice = \"image\"\n",
        "        return choice, content\n",
        "\n",
        "    else:\n",
        "        raise gr.Error(\"Oops, unsupported files.\")\n",
        "\n",
        "\n",
        "@spaces.GPU()\n",
        "def stream_chat(message, history: list, temperature: float, max_length: int, top_p: float, top_k: int, penalty: float):\n",
        "    print(f'message is - {message}')\n",
        "    print(f'history is - {history}')\n",
        "    conversation = []\n",
        "    prompt_files = []\n",
        "    if message[\"files\"]:\n",
        "        choice, contents = mode_load(message[\"files\"][-1])\n",
        "        if choice == \"image\":\n",
        "            conversation.append({\"role\": \"user\", \"image\": contents, \"content\": message['text']})\n",
        "        elif choice == \"doc\":\n",
        "            format_msg = contents + \"\\n\\n\\n\" + \"{} files uploaded.\\n\" + message['text']\n",
        "            conversation.append({\"role\": \"user\", \"content\": format_msg})\n",
        "    else:\n",
        "        if len(history) == 0:\n",
        "            # raise gr.Error(\"Please upload an image first.\")\n",
        "            contents = None\n",
        "            conversation.append({\"role\": \"user\", \"content\": message['text']})\n",
        "        else:\n",
        "            # image = Image.open(history[0][0][0])\n",
        "            for prompt, answer in history:\n",
        "                if answer is None:\n",
        "                    prompt_files.append(prompt[0])\n",
        "                    conversation.extend([{\"role\": \"user\", \"content\": \"\"}, {\"role\": \"assistant\", \"content\": \"\"}])\n",
        "                else:\n",
        "                    conversation.extend([{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": answer}])\n",
        "            if len(prompt_files) > 0:\n",
        "                choice, contents = mode_load(prompt_files[-1])\n",
        "            else:\n",
        "                choice = \"\"\n",
        "                conversation.append({\"role\": \"user\", \"image\": \"\", \"content\": message['text']})\n",
        "\n",
        "\n",
        "            if choice == \"image\":\n",
        "                conversation.append({\"role\": \"user\", \"image\": contents, \"content\": message['text']})\n",
        "            elif choice == \"doc\":\n",
        "                format_msg = contents + \"\\n\\n\\n\" + \"{} files uploaded.\\n\" + message['text']\n",
        "                conversation.append({\"role\": \"user\", \"content\": format_msg})\n",
        "    print(f\"Conversation is -\\n{conversation}\")\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(conversation, tokenize=True, add_generation_prompt=True,\n",
        "                                              return_tensors=\"pt\", return_dict=True).to(model.device)\n",
        "    streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "        max_length=max_length,\n",
        "        streamer=streamer,\n",
        "        do_sample=True,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        temperature=temperature,\n",
        "        repetition_penalty=penalty,\n",
        "        eos_token_id=[151329, 151336, 151338],\n",
        "    )\n",
        "    gen_kwargs = {**input_ids, **generate_kwargs}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        thread = Thread(target=model.generate, kwargs=gen_kwargs)\n",
        "        thread.start()\n",
        "        buffer = \"\"\n",
        "        for new_text in streamer:\n",
        "            buffer += new_text\n",
        "            yield buffer\n",
        "\n",
        "\n",
        "chatbot = gr.Chatbot(\n",
        "    #rtl=True\n",
        ")\n",
        "chat_input = gr.MultimodalTextbox(\n",
        "    interactive=True,\n",
        "    placeholder=\"Enter message or upload a file ...\",\n",
        "    show_label=False,\n",
        "    #rtl=True,\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "EXAMPLES = [\n",
        "    [{\"text\": \"Write a poem about spring season in French Language\", }],\n",
        "    [{\"text\": \"what does this chart mean?\", \"files\": [\"sales.png\"]}],\n",
        "    [{\"text\": \"¬øQu√© est√° escrito a mano en esta foto?\", \"files\": [\"receipt1.png\"]}],\n",
        "    [{\"text\": \"ÿØÿ± ŸÖŸàÿ±ÿØ ÿß€åŸÜ ÿπ⁄©ÿ≥ ÿ™Ÿàÿ∂€åÿ≠ ÿ®ÿØŸá Ÿà ÿ®⁄ØŸà ÿß€åŸÜ ⁄ÜŸá ŸÅÿµŸÑ€å ŸÖ€å ÿ™ŸàÿßŸÜÿØ ÿ®ÿßÿ¥ÿØ\", \"files\": [\"nature.jpg\"]}]\n",
        "]\n",
        "\n",
        "with gr.Blocks(css=CSS, theme=\"soft\", fill_height=True) as demo:\n",
        "    gr.HTML(TITLE)\n",
        "    gr.HTML(DESCRIPTION)\n",
        "    gr.ChatInterface(\n",
        "        fn=stream_chat,\n",
        "        multimodal=True,\n",
        "\n",
        "\n",
        "        textbox=chat_input,\n",
        "        chatbot=chatbot,\n",
        "        fill_height=True,\n",
        "        additional_inputs_accordion=gr.Accordion(label=\"‚öôÔ∏è Parameters\", open=False, render=False),\n",
        "        additional_inputs=[\n",
        "            gr.Slider(\n",
        "                minimum=0,\n",
        "                maximum=1,\n",
        "                step=0.1,\n",
        "                value=0.8,\n",
        "                label=\"Temperature\",\n",
        "                render=False,\n",
        "            ),\n",
        "            gr.Slider(\n",
        "                minimum=1024,\n",
        "                maximum=8192,\n",
        "                step=1,\n",
        "                value=4096,\n",
        "                label=\"Max Length\",\n",
        "                render=False,\n",
        "            ),\n",
        "            gr.Slider(\n",
        "                minimum=0.0,\n",
        "                maximum=1.0,\n",
        "                step=0.1,\n",
        "                value=1.0,\n",
        "                label=\"top_p\",\n",
        "                render=False,\n",
        "            ),\n",
        "            gr.Slider(\n",
        "                minimum=1,\n",
        "                maximum=20,\n",
        "                step=1,\n",
        "                value=10,\n",
        "                label=\"top_k\",\n",
        "                render=False,\n",
        "            ),\n",
        "            gr.Slider(\n",
        "                minimum=0.0,\n",
        "                maximum=2.0,\n",
        "                step=0.1,\n",
        "                value=1.0,\n",
        "                label=\"Repetition penalty\",\n",
        "                render=False,\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    gr.Examples(EXAMPLES, [chat_input])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue(api_open=False).launch(show_api=False, share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyODMiXFLOfaTEBDTQowV4OF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}